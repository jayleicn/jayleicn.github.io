<!doctype html>
<html lang="en">
<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <!--  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">-->
  <meta name="viewport" content="width=1024">
  <meta http-equiv='cache-control' content='no-cache'> 
  <meta http-equiv='expires' content='0'> 
  <meta http-equiv='pragma' content='no-cache'>

  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css">
  <link rel="stylesheet" href="style.css">

  <script src="https://code.jquery.com/jquery-3.4.1.slim.min.js"
          integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n"
          crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
          integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
          crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js"
          integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6"
          crossorigin="anonymous"></script>

  <!-- Place this tag in your head or just before your close body tag. -->
  <script async defer src="https://buttons.github.io/buttons.js"></script>

  <title>Jie Lei</title>
</head>
<body>
<div class="main">
  <div class="container"></div>

  <div class="name">
    <h1>Jie Lei</h1>
  </div>

  <div class="header">

    <table class="table-header">
      <tbody>
      <tr>
        <td align="top">
          <img class="me" src="images/jielei_lake_sammamish_crop.jpg" alt="Jie Lei" style="width:200px;">
          <!-- <p style="font-size: x-small">Old Town, San Diego, May 2019 (Courtesy of Qin)</p> -->
          <p style="font-size: x-small">Lake Sammamish, Bellevue, Aug 2023</p>
        </td>
        <td align="left">
          <p>I am a research scientist at <a href="https://ai.facebook.com/">Meta AI</a>, Seattle. 
            My primary research interests are vision-and-language and video modeling.
            I received my PhD in Computer Science from UNC Chapel Hill in 2022, 
            advised by <a target="_blank" href="http://tamaraberg.com/">Tamara L. Berg</a>
            and <a target="_blank" href="https://www.cs.unc.edu/~mbansal">Mohit Bansal</a>.
            I received my bachelor's degree in Computer Science from Yingcai Honors College, <a
              href="http://www.uestc.edu.cn/">University
            of Electronic Science and Technology of China</a> (UESTC) in 2017.
            I am a receipt of the <a href="https://research.adobe.com/fellowship/previous-fellowship-award-winners/">Adobe Research Fellowship</a> and the <a
                href="http://cvpr2021.thecvf.com/node/329">CVPR 2021 Best Student Paper Honorable Mention</a> award.
          </p>

          <p>
            <a href="https://scholar.google.com/citations?user=SZN9FLIAAAAJ&hl=en"><img
                src="./images/google_scholar_icon.png" alt="google scholar" class="icon"></a>
            <a href="https://github.com/jayleicn"><img src="./images/github_icon.png" alt="github" class="icon"></a>
            <a href="https://twitter.com/jayleicn"><img src="./images/twitter_icon.png" alt="twitter" class="icon"></a>
            <a href="cv/cv_jielei.pdf"><img src="./images/cv_icon.png" alt="cv" class="icon"></a>
          </p>
          <p><b>Email</b>: jielei [at] meta.com
          </p>
        </td>
      </tr>
      </tbody>
    </table>
  </div>

  <hr style="margin: 0">

  <!-- <p style="margin-top: 10pt; ">Our team at Meta is hiring 2023 research scientist interns in image/video/text generation, vision-and-language, video/image understanding, etc. Check job descriptions <a href="https://www.metacareers.com/v2/jobs/527447392667117/">[1] Computer Vision</a>, <a href="https://www.metacareers.com/v2/jobs/2048946418633964/">[2] Natural Language Processing</a>.</p> -->

  <div class="news">
    <h2>News</h2>
    <ul>  
      <li><span>Feb 2023</span> &raquo; Two papers accepted at CVPR 2023.</li>
      <li><span>Feb 2023</span> &raquo; Our tutorial <a href="https://blender.cs.illinois.edu/tutorial/KnowledgeVLP/">Knowledge-Driven Vision-Language Pretraining</a> is accepted at CVPR 2023, see you in Vancouver.</li>
      <li><span>Dec 2022</span> &raquo; Our tutorial <a href="https://blender.cs.illinois.edu/tutorial/knowledgeVLP/">Knowledge-Driven Vision-Language Pretraining</a> is accepted at AAAI 2023.</li>
      <li><span>May 2022</span> &raquo; I graduated with a PhD in Computer Science from UNC.</li>
      <li><span>Mar 2022</span> &raquo; Our workshop <a href="https://sites.google.com/view/t4v-cvpr22">T4V: Transformers for Vision</a> is accepted
         at CVPR 2022.</li>
      <li><span>Jun 2021</span> &raquo; We are hosting <a href="https://value-benchmark.github.io/challenge_2021.html">VALUE Challenge</a> for video and language understanding at ICCV 2021
        <a href="https://sites.google.com/view/iccv21clvl/">CLCV workshop</a>, please join!</li>
      <li><span>Jun 2021</span> &raquo; <a href="https://arxiv.org/abs/2102.06183">ClipBERT</a> is awarded the <a
          href="http://cvpr2021.thecvf.com/node/329">CVPR 2021 Best Student Paper Honorable Mention</a>! &#128525;
      </li>
      <!-- <li><span>Mar 2021</span> &raquo; <a href="https://arxiv.org/abs/2102.06183">ClipBERT</a> is accepted as an oral -->
        <!-- paper at CVPR 2021. -->
      <!-- </li> -->
      <li><span>Feb 2021</span> &raquo; Received <a
          href="https://research.adobe.com/fellowship/previous-fellowship-award-winners/">Adobe Research Fellowship</a>,
        thanks Adobe!
      </li>
      <li><span>Jan 2021</span> &raquo; Research Internship @Facebook AI, working with <a href="https://lichengunc.github.io/">Licheng Yu</a>,
        <a href="https://xinleic.xyz/">Xinlei Chen</a> and <a href="https://n-zhang.github.io/">Ning Zhang</a>.</li>
      <li><span>May 2020</span> &raquo; Research Internship @Microsoft, working with
        <a href="https://scholar.google.com/citations?user=WR875gYAAAAJ&hl=en">Linjie Li</a>, <a
            href="https://luoweizhou.github.io/">Luowei Zhou</a>, <a href="http://zhegan27.github.io/">Zhe Gan</a> and
        <a href="https://www.linkedin.com/in/jingjing-liu-65703431/">Jingjing Liu</a>.
      </li>
      <!-- <li><span>Jan 2020</span> &raquo; Released <a href="https://tvr.cs.unc.edu/">TVR/TVC Datasets</a> for large-scale -->
        <!-- video-subtitle moment retrieval and captioning, -->
      <!-- </li> -->
      <li><span>May 2019</span> &raquo; Research Internship @Tencent AI Lab, Seattle, with
        <a href="https://lwwangcse.github.io/">Liwei Wang</a>, <a
            href="https://scholar.google.com/citations?hl=en&user=S6OFEFEAAAAJ">Yelong Shen</a> and
        <a href="https://scholar.google.com/citations?user=tMY31_gAAAAJ&hl=en">Dong Yu</a></li>
      <!-- <li><span>Sep 2018</span> &raquo; <a target="_blank" href="http://tvqa.cs.unc.edu/">TVQA Dataset</a> is released. -->
      </li>
      <li><span>Aug 2017</span> &raquo; I joined <a target="_blank" href="http://www.unc.edu/">UNC</a> as a PhD student.
      </li>
    </ul>
  </div>

  <!--<hr>-->


  <div class="papers">
    <h2>Publications & Preprints</h2>

    <!--Singularity-->
    <div class="row my-3">
      <div class="col-lg-12 my-auto">
        <div class="paper-text">
          <div class="paper-title">
            Revealing Single Frame Bias for Video-and-Language Learning
          </div>
          <div class="paper-authors">
            <b>Jie Lei</b>, Tamara L. Berg, Mohit Bansal
          </div>

          <div class="paper-meta">
            <span class="badge badge-info">ACL 2023</span>
            <a target="_blank" href="https://arxiv.org/abs/2206.03428">[PDF]</a>
            <a target="_blank" href="https://github.com/jayleicn/singularity">[Data & Code]</a>
            <a class="github-button" href="https://github.com/jayleicn/singularity" data-icon="octicon-star"
               data-show-count="true" aria-label="Star jayleicn/singularity on GitHub">Star</a>
          </div>
        </div>
      </div>
    </div>

    <!--VindLU-->
    <div class="row my-3">
      <div class="col-lg-12 my-auto">
        <div class="paper-text">
          <div class="paper-title">
            VindLU: A Recipe for Effective Video-and-Language Pretraining
          </div>
          <div class="paper-authors">
            Feng Cheng, Xizi Wang, <b>Jie Lei</b>, David Crandall, Mohit Bansal, Gedas Bertasius
          </div>

          <div class="paper-meta">
            <span class="badge badge-info">CVPR 2023</span>
            <a target="_blank" href="https://arxiv.org/abs/2212.05051">[PDF]</a>
            <a target="_blank" href="https://github.com/klauscc/VindLU">[Code]</a>
            <a class="github-button" href="https://github.com/klauscc/VindLU" data-icon="octicon-star"
               data-show-count="true" aria-label="Star klauscc/VindLU on GitHub">Star</a>            
          </div>
        </div>
      </div>
    </div>        


    <!--LAVISH-->
    <div class="row my-3">
      <div class="col-lg-12 my-auto">
        <div class="paper-text">
          <div class="paper-title">
            Vision Transformers are Parameter-Efficient Audio-Visual Learners
          </div>
          <div class="paper-authors">
            Yan-Bo Lin, Yi-Lin Sung, <b>Jie Lei</b>, Mohit Bansal, Gedas Bertasius
          </div>

          <div class="paper-meta">
            <span class="badge badge-info">CVPR 2023</span>
            <a target="_blank" href="https://arxiv.org/abs/2212.07983">[PDF]</a>
            <a target="_blank" href="https://github.com/GenjiB/LAVISH">[Code]</a>
            <a class="github-button" href="https://github.com/GenjiB/LAVISH" data-icon="octicon-star"
               data-show-count="true" aria-label="Star GenjiB/LAVISH on GitHub">Star</a>                
          </div>
        </div>
      </div>
    </div>    

    <!--PERCEIVER-VL-->
    <div class="row my-3">
      <div class="col-lg-12 my-auto">
        <div class="paper-text">
          <div class="paper-title">
            PERCEIVER-VL: Efficient Vision-and-Language Modeling with Iterative Latent Attention
          </div>
          <div class="paper-authors">
            Zineng Tang*, Jaemin Cho*, <b>Jie Lei</b>, Mohit Bansal 
          </div>

          <div class="paper-meta">
            <span class="badge badge-info">WACV 2023</span>
            <a target="_blank" href="https://arxiv.org/abs/2211.11701">[PDF]</a>
            <a target="_blank" href="https://github.com/zinengtang/Perceiver_VL">[Code]</a>
            <a class="github-button" href="https://github.com/zinengtang/Perceiver_VL" data-icon="octicon-star"
               data-show-count="true" aria-label="Star zinengtang/Perceiver_VL on GitHub">Star</a>                 
          </div>
        </div>
      </div>
    </div>

    <!--GPT3Speaker-->
    <div class="row my-3">
      <div class="col-lg-12 my-auto">
        <div class="paper-text">
          <div class="paper-title">
            Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners
          </div>
          <div class="paper-authors">
            Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou, <b>Jie Lei</b>, Xudong Lin, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, Shih-Fu Chang, Mohit Bansal, Heng Ji
          </div>

          <div class="paper-meta">
            <span class="badge badge-info">NeurIPS 2022</span>
            <a target="_blank" href="https://arxiv.org/abs/2205.10747">[PDF]</a>
            <a target="_blank" href="https://github.com/MikeWangWZHL/VidIL">[Code]</a>
            <a class="github-button" href="https://github.com/MikeWangWZHL/VidIL" data-icon="octicon-star"
               data-show-count="true" aria-label="Star MikeWangWZHL/VidIL on GitHub">Star</a>                
          </div>
        </div>
      </div>
    </div>

    <!--Eclipse-->
    <div class="row my-3">
      <div class="col-lg-12 my-auto">
        <div class="paper-text">
          <div class="paper-title">
            ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound
          </div>
          <div class="paper-authors">
            Yan-Bo Lin, <b>Jie Lei</b>, Mohit Bansal, Gedas Bertasius
          </div>

          <div class="paper-meta">
            <span class="badge badge-info">ECCV 2022</span>
            <span class="badge badge-success">Oral</span>
            <a target="_blank" href="https://arxiv.org/abs/2204.02874">[PDF]</a>
            <a target="_blank" href="https://yanbo.ml/project_page/eclipse/">[Project Page]</a>
            <a target="_blank" href="https://github.com/GenjiB/ECLIPSE">[Code]</a>
            <a class="github-button" href="https://github.com/GenjiB/ECLIPSE" data-icon="octicon-star"
               data-show-count="true" aria-label="Star GenjiB/ECLIPSE on GitHub">Star</a>     
          </div>
        </div>
      </div>
    </div>

    <!--demo-->
    <div class="row my-3">
      <div class="col-lg-12 my-auto">
        <div class="paper-text">
          <div class="paper-title">
            Resin-11: Schema-guided event prediction for 11 newsworthy scenarios
          </div>
          <div class="paper-authors">
            Xinya Du, Zixuan Zhang, Sha Li, Pengfei Yu, Hongwei Wang, Tuan Lai, Xudong Lin, Ziqi Wang, Iris Liu, Ben Zhou, Haoyang Wen, Manling Li, Darryl Hannan, <b>Jie Lei</b>, Hyounghun Kim, Rotem Dror, Haoyu Wang, Michael Regan, Qi Zeng, Qing Lyu, Charles Yu, Carl Edwards, Xiaomeng Jin, Yizhu Jiao, Ghazaleh Kazeminejad, Zhenhailong Wang, Chris Callison-Burch, Mohit Bansal, Carl Vondrick, Jiawei Han, Dan Roth, Shih-Fu Chang, Martha Palmer, Heng Ji            
          </div>

          <div class="paper-meta">
            <span class="badge badge-info">NAACL 2022 System Demo</span>
            <a target="_blank" href="https://aclanthology.org/2022.naacl-demo.7/">[PDF]</a>
          </div>
        </div>
      </div>
    </div>

    <!--LoopITR-->
    <div class="row my-3">
      <div class="col-lg-12 my-auto">
        <div class="paper-text">
          <div class="paper-title">
            LoopITR: Combining Dual and Cross Encoder Architectures for Image-Text Retrieval
          </div>
          <div class="paper-authors">
            <b>Jie Lei</b>, Xinlei Chen, Ning Zhang, Mengjiao Wang, Mohit Bansal, Tamara L. Berg, Licheng Yu
          </div>

          <div class="paper-meta">
            <span class="badge badge-info">arXiv 2022</span>
            <a target="_blank" href="https://arxiv.org/abs/2203.05465">[PDF]</a>
          </div>
        </div>
      </div>
    </div>

    <!--VIMPAC-->
    <div class="row my-3">
      <div class="col-lg-12 my-auto">
        <div class="paper-text">
          <div class="paper-title">
            VIMPAC: Video Pre-Training via Masked Token Prediction and Contrastive Learning
          </div>
          <div class="paper-authors">
            Hao Tan*, <b>Jie Lei*</b>, Thomas Wolf, Mohit Bansal
          </div>

          <div class="paper-meta">
            <span class="badge badge-info">CVPRW 2022</span>
            <a target="_blank" href="https://arxiv.org/abs/2106.11250">[PDF]</a>
            <a target="_blank" href="https://github.com/airsplay/vimpac">[Code]</a>
            <a class="github-button" href="https://github.com/airsplay/vimpac" data-icon="octicon-star"
               data-show-count="true" aria-label="Star airsplay/vimpac on GitHub">Star</a>
          </div>
        </div>
      </div>
    </div>

    <!--MomentDETR-->
    <div class="row my-3">
      <div class="col-lg-12 my-auto">
        <div class="paper-text">
          <div class="paper-title">
            QVHighlights: Detecting Moments and Highlights in Videos via Natural Language Queries
          </div>
          <div class="paper-authors">
            <b>Jie Lei</b>, Tamara L. Berg, Mohit Bansal
          </div>

          <div class="paper-meta">
            <span class="badge badge-info">NeurIPS 2021</span>
            <a target="_blank" href="https://arxiv.org/abs/2107.09609">[PDF]</a>
            <a target="_blank" href="https://github.com/jayleicn/moment_detr">[Dataset & Moment-DETR Code]</a>
            <a class="github-button" href="https://github.com/jayleicn/moment_detr" data-icon="octicon-star"
               data-show-count="true" aria-label="Star jayleicn/moment_detr on GitHub">Star</a>
          </div>
        </div>
      </div>
    </div>

    <!--VALUE-->
    <div class="row my-3">
      <div class="col-lg-12 my-auto">
        <div class="paper-text">
          <div class="paper-title">
            VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation
          </div>
          <div class="paper-authors">
            Linjie Li*, <b>Jie Lei*</b>, Zhe Gan, Licheng Yu, Yen-Chun Chen, Rohit Pillai, Yu Cheng, Luowei Zhou, Xin
            Eric Wang, William Yang Wang, Tamara L. Berg, Mohit Bansal, Jingjing Liu, Lijuan Wang, Zicheng Liu
          </div>

          <div class="paper-meta">
            <span class="badge badge-info">NeurIPS 2021 - Datasets and Benchmarks Track</span>
            <a target="_blank" href="https://arxiv.org/abs/2106.04632">[PDF]</a>
            <a target="_blank" href="https://github.com/VALUE-Leaderboard/StarterCode">[Code]</a>
            <a target="_blank" href="https://value-leaderboard.github.io/">[Leaderboard & Challenge]</a>
          </div>
        </div>
      </div>
    </div>

    <!--Adv VQA-->
    <div class="row my-3">
      <div class="col-lg-12 my-auto">
        <div class="paper-text">
          <div class="paper-title">
            Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models
          </div>
          <div class="paper-authors">
            Linjie Li, <b>Jie Lei</b>, Zhe Gan, Jingjing Liu
          </div>

          <div class="paper-meta">
            <span class="badge badge-info">ICCV 2021</span>
            <span class="badge badge-success">Oral (top 3%)</span>
            <a target="_blank" href="https://arxiv.org/abs/2106.00245">[PDF]</a>
            <a target="_blank" href="https://adversarialvqa.github.io/">[Dataset]</a>
          </div>
        </div>
      </div>
    </div>

    <!--mTVR-->
    <div class="row my-3">
      <div class="col-lg-12 my-auto">
        <div class="paper-text">

          <div class="paper-title">
            mTVR: Multilingual Moment Retrieval in Videos
          </div>

          <div class="paper-authors">
            <b>Jie Lei</b>, Tamara L. Berg, Mohit Bansal
          </div>

          <div class="paper-meta">
            <span class="badge badge-info">ACL 2021</span>
            <a target="_blank" href="https://arxiv.org/abs/2108.00061">[PDF]</a>
            <a target="_blank" href="https://github.com/jayleicn/mTVRetrieval">[Code]</a>
          </div>

        </div>
      </div>
    </div>


    <!--VL-T5-->
    <div class="row my-3">
      <div class="col-lg-12 my-auto my-auto">
        <div class="paper-text">
          <div class="paper-title">
            Unifying Vision-and-Language Tasks via Text Generation
          </div>
          <div class="paper-authors">
            Jaemin Cho, <b>Jie Lei</b>, Hao Tan, Mohit Bansal
          </div>

          <div class="paper-meta">
            <span class="badge badge-info">ICML 2021</span>
            <a target="_blank" href="https://arxiv.org/abs/2102.02779">[PDF]</a>
            <a target="_blank" href="https://github.com/j-min/VL-T5">[Code]</a>
            <a class="github-button" href="https://github.com/j-min/VL-T5" data-icon="octicon-star"
               data-show-count="true" aria-label="Star j-min/VL-T5 on GitHub">Star</a>
          </div>
        </div>
      </div>
    </div>


    <!--DECEMBERT-->
    <div class="row my-3">
      <div class="col-lg-12 my-auto">
        <div class="paper-text">
          <div class="paper-title">
            Improved Pre-Training from Noisy Instructional Videos via Dense Captions and Entropy Minimization
          </div>
          <div class="paper-authors">
            Zineng Tang*, <b>Jie Lei*</b>, Mohit Bansal
          </div>

          <div class="paper-meta">
            <span class="badge badge-info">NAACL 2021</span>
            <a target="_blank" href="https://www.aclweb.org/anthology/2021.naacl-main.193/">[PDF]</a>
            <a target="_blank" href="https://github.com/zinengtang/DeCEMBERT">[Code]</a>
            <a class="github-button" href="https://github.com/zinengtang/DeCEMBERT" data-icon="octicon-star"
               data-show-count="true" aria-label="Star zinengtang/DeCEMBERT on GitHub">Star</a>
          </div>
        </div>
      </div>
    </div>


    <!--ClipBERT-->
    <div class="row my-3">
      <div class="col-lg-12 my-auto">
        <div class="paper-text">

          <div class="paper-title">
            Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling
          </div>

          <div class="paper-authors">
            <b>Jie Lei</b>*, Linjie Li*, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal, Jingjing Liu
          </div>

          <div class="paper-meta">
            <span class="badge badge-info">CVPR 2021</span> <span class="badge badge-success">Best Student Paper Honorable Mention (top 0.1%)</span>
            <span class="badge badge-success">Oral</span>
            <a target="_blank" href="https://arxiv.org/abs/2102.06183">[PDF]</a>
            <a target="_blank" href="https://github.com/jayleicn/ClipBERT">[Code]</a>
            <a class="github-button" href="https://github.com/jayleicn/ClipBERT" data-icon="octicon-star"
               data-show-count="true" aria-label="Star jayleicn/ClipBERT on GitHub">Star</a>
          </div>

        </div>
      </div>
    </div>

    <!--VLEP-->
    <div class="row my-3">
      <div class="col-lg-12 my-auto">
        <div class="paper-text">
          <div class="paper-title">
            What is More Likely to Happen Next? Video-and-Language Future Event Prediction
          </div>


          <div class="paper-authors">
            <b>Jie Lei</b>, Licheng Yu, Tamara L. Berg, Mohit Bansal
          </div>

          <div class="paper-meta">
            <span class="badge badge-info">EMNLP 2020</span>
            <a target="_blank" href="https://arxiv.org/abs/2010.07999">[PDF]</a>
            <a target="_blank" href="#"></a>
            <a target="_blank" href="https://github.com/jayleicn/VideoLanguageFuturePred">[VLEP Dataset]</a>
            <!--            <a class="github-button" href="https://github.com/jayleicn/VideoLanguageFuturePred" data-icon="octicon-star"-->
            <!--               data-show-count="true" aria-label="Star jayleicn/VideoLanguageFuturePred on GitHub">Star</a>-->
          </div>

        </div>
      </div>
    </div>

    <!--TVR-->
    <div class="row my-3">
      <div class="col-lg-12 my-auto">
        <div class="paper-text">
          <div class="paper-title">
            TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval
          </div>

          <div class="paper-authors">
            <b>Jie Lei</b>, Licheng Yu, Tamara L. Berg, Mohit Bansal
          </div>


          <div class="paper-meta">
            <span class="badge badge-info">ECCV 2020</span>
            <a target="_blank" href="https://arxiv.org/abs/2001.09099">[PDF]</a>
            <a target="_blank" href="https://tvr.cs.unc.edu/">[TVR/TVC Datasets]</a>
            <a target="_blank" href="./files/eccv20_tvr_talk.pptx">[Slides]</a>
            <br>
            <a target="_blank" href="https://github.com/jayleicn/TVRetrieval">[Retrieval Code]</a>
            <a class="github-button" href="https://github.com/jayleicn/TVRetrieval" data-icon="octicon-star"
               data-show-count="true" aria-label="Star jayleicn/TVRetrieval on GitHub">Star</a>
            <a target="_blank" href="https://github.com/jayleicn/TVCaption">[Captioning Code]</a>
            <a class="github-button" href="https://github.com/jayleicn/TVCaption" data-icon="octicon-star"
               data-show-count="true" aria-label="Star jayleicn/TVCaption on GitHub">Star</a>
          </div>

        </div>
      </div>
    </div>

    <!--MART-->
    <div class="row mb-3">
      <div class="col-lg-12 my-auto">
        <div class="paper-text">
          <div class="paper-title">
            MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning
          </div>

          <div class="paper-authors">
            <b>Jie Lei</b>, Liwei Wang, Yelong Shen, Dong Yu, Tamara L. Berg, Mohit Bansal
          </div>
          <div class="paper-meta">
            <span class="badge badge-info">ACL 2020</span>
            <a href="https://arxiv.org/abs/2005.05402">[PDF]</a>
            <a href="./files/acl20_mart_talk.pptx">[Slides]</a>
            <a href="https://github.com/jayleicn/recurrent-transformer">[Code]</a>
            <a class="github-button" href="https://github.com/jayleicn/recurrent-transformer"
               data-icon="octicon-star" data-show-count="true"
               aria-label="Star jayleicn/recurrent-transformer on GitHub">Star</a>

          </div>
        </div>
      </div>
    </div>

    <!--TVQA PLUS-->
    <div class="row mb-3">
      <div class="col-lg-12 my-auto">
        <div class="paper-text">
          <div class="paper-title">
            TVQA+: Spatio-Temporal Grounding for Video Question Answering
          </div>


          <div class="paper-authors">
            <b>Jie Lei</b>, Licheng Yu, Tamara L. Berg, Mohit Bansal
          </div>
          <div class="paper-meta">
            <span class="badge badge-info">ACL 2020</span>
            <a target="_blank" href="https://arxiv.org/abs/1904.11574">[PDF]</a>
            <a href="./files/acl20_tvqa_plus_talk.pptx">[Slides]</a>
            <a target="_blank" href="http://tvqa.cs.unc.edu/">[Dataset]</a>
            <a target="_blank" href="https://github.com/jayleicn/TVQA-PLUS">[Code]</a>
            <a class="github-button" href="https://github.com/jayleicn/TVQAplus"
               data-icon="octicon-star" data-show-count="true"
               aria-label="Star jayleicn/recurrent-transformer on GitHub">Star</a>
          </div>
        </div>
      </div>
    </div>

    <!--TVQA-->
    <div class="row mb-3">
      <div class="col-lg-12 my-auto">
        <div class="paper-text">
          <div class="paper-title">
            TVQA: Localized, Compositional Video Question Answering
          </div>

          <div class="paper-authors">
            <b>Jie Lei</b>, Licheng Yu, Mohit Bansal, Tamara L. Berg
          </div>

          <div class="paper-meta">
            <span class="badge badge-info">EMNLP 2018</span>
            <span class="badge badge-success">Oral</span>
            <a target="_blank" href="https://arxiv.org/abs/1809.01696">[PDF]</a>
            <a href="./files/tvqa_emnlp18_oral.pptx">[Slides]</a>
            <a target="_blank" href="http://tvqa.cs.unc.edu/">[Dataset]</a>
            <a target="_blank" href="https://github.com/jayleicn/TVQA">[Code]</a>
            <a class="github-button" href="https://github.com/jayleicn/TVQA" data-icon="octicon-star"
               data-show-count="true" aria-label="Star jayleicn/TVQA on GitHub">Star</a>
          </div>
        </div>
      </div>

    </div>

    <!--CRV classification-->
    <div class="row mb-3">
      <div class="col-lg-12 my-auto">
        <div class="paper-text">
          <div class="paper-title">
            Weakly Supervised Image Classification with Coarse and Fine Labels
          </div>

        </div>
        <div class="paper-authors">
          <b>Jie Lei</b>, Zhenyu Guo and Yang Wang
        </div>

        <div class="paper-meta">
          <span class="badge badge-info">CRV 2017</span>
          <a target="_blank" href="pdfs/crv17_classification.pdf">[PDF]</a>
          <a target="_blank" href="https://github.com/jayleicn/classification-with-coarse-fine-labels">[Code]</a>
          <a class="github-button" href="https://github.com/jayleicn/classification-with-coarse-fine-labels"
             data-icon="octicon-star"
             data-show-count="true" aria-label="Star jayleicn/classification-with-coarse-fine-labels on GitHub">Star</a>
        </div>

      </div>
    </div>
</div>


<div class="projects">
  <h2>Projects</h2>
  <div class="row my-3">
    <div class="col-lg-12 my-auto">
      <div class="paper-text">
        <div class="paper-title">
          AnimeGAN: Create Anime Face using Generative Adversarial Networks, 
        </div> 
        <div class="paper-authors">
          <b>Jie Lei</b>
        </div>
        <div class="paper-conference">
          A simple GAN model that could automatically generate anime girl faces.
        </div>

        <div class="paper-meta">
          <span class="badge badge-info">GitHub 2017</span>
          <a target="_blank" href="https://github.com/jayleicn/animeGAN">[Code & AnimeFace Dataset]</a>
          <a class="github-button" href="https://github.com/jayleicn/animeGAN" data-icon="octicon-star"
             data-show-count="true" aria-label="Star jayleicn/animeGAN on GitHub">Star</a>
        </div>
      </div>
    </div>
  </div>
</div>

<!--<hr>-->

<div class="misc">
  <h2>Miscs</h2>
  <ul>
    <li>My Chinese name is 雷杰. I am from <a href="https://en.wikipedia.org/wiki/Sichuan">Sichuan</a>, the hometown of
      pandas.
    </li>
    <li>I am <strike>a big fan</strike> of <i>Attack on Titan</i>.</li>
  </ul>
</div>


</div>
</body>
</html>
